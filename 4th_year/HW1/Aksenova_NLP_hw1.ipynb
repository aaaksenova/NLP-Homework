{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aksenova_NLP_hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0cgE63K7itudytwzIcE2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaaksenova/NLP-Homework/blob/main/Aksenova_NLP_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmsTzwWIEeeU"
      },
      "source": [
        "# Key Words Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy1vgMd6Emut"
      },
      "source": [
        "To-do:\n",
        "\n",
        "(1 балл) Подготовить мини-корпус (не меньше 4 текстов, \n",
        "примерный общий объём - 3-5 тысяч токенов) с разметкой ключевых слов. Предполагается, что вы найдете источник текстов, в котором уже выделены ключевые слова. Укажите источник корпуса и опишите, в каком виде там были представлены ключевые слова. **(DONE)**\n",
        "\n",
        "(2 балла) Разметить ключевые слова самостоятельно. Оценить пересечение с имеющейся разметкой. Составить эталон разметки (например, пересечение или объединение вашей разметки и исходной). **(DONE)**\n",
        "\n",
        "(2 балла) Применить к этому корпусу 3 метода извлечения ключевых слов на выбор (RAKE, TextRank, tf\\*idf, OKAPI BM25, ...) **(DONE)**\n",
        "\n",
        "(2 балла) Составить морфологические/синтаксические шаблоны для ключевых слов и фраз, выделить соответствующие им подстроки из корпуса (например, именные группы Adj+Noun). Применить эти фильтры к спискам ключевых слов.\n",
        "\n",
        "(2 балла) Оценить точность, полноту, F-меру выбранных методов относительно эталона: с учётом морфосинтаксических шаблонов и без них. **(DONE)**\n",
        "\n",
        "(1 балл) Описать ошибки автоматического выделения ключевых слов (что выделяется лишнее, что не выделяется); предложить свои методы решения этих проблем. **(DONE)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JAaGAjw20w3",
        "outputId": "22be7107-83d5-41be-93f6-b98d3d477700"
      },
      "source": [
        "! pip install spacy --upgrade\n",
        "! python -m spacy download ru_core_news_md\n",
        "! pip install python-rake\n",
        "! pip install summa\n",
        "! pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.1.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 28.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-legacy, pathy, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 pathy-0.6.1 pydantic-1.8.2 spacy-3.1.4 spacy-legacy-3.0.8 srsly-2.4.2 thinc-8.0.12 typer-0.4.0\n",
            "Collecting ru-core-news-md==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.1.0/ru_core_news_md-3.1.0-py3-none-any.whl (42.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.7 MB 2.3 kB/s \n",
            "\u001b[?25hCollecting pymorphy2>=0.9\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from ru-core-news-md==3.1.0) (3.1.4)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.9->ru-core-news-md==3.1.0) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 17.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (21.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (8.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->ru-core-news-md==3.1.0) (2.0.1)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2, ru-core-news-md\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 ru-core-news-md-3.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_md')\n",
            "Collecting python-rake\n",
            "  Downloading python_rake-1.5.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: python-rake\n",
            "Successfully installed python-rake-1.5.0\n",
            "Collecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54410 sha256=854d6a7cb2c6b5c820729aba86653a9d03a20bf357a63de0c615793c2b5d9c4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/64/ac/7b443477588d365ef37ada30d456bdf5f07dc5be9f6324cb6e\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 22.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers>=0.10.3\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=eb57b3bc7adc5d00876430204dc25f46d6adead2cbe70ea9b8ae2d156a89f7a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMCGVL1wJULu",
        "outputId": "cea41a26-dc39-4130-9d3c-a84a39afa218"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import RAKE\n",
        "from summa import keywords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaJH2EyjFgLv"
      },
      "source": [
        "## Corpus Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJaxSVLcopvB"
      },
      "source": [
        "with open('key_papers.txt') as fh:\n",
        "    fil = fh.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvk_Y6Nx782E"
      },
      "source": [
        "papers = fil.split('Литература\\n')[2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTmU5N2V8-qe",
        "outputId": "aef5740b-dcb2-480b-b338-325b888223d1"
      },
      "source": [
        "len(papers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na-VZMrR9A3e"
      },
      "source": [
        "key_to_paper = [(i.split('Ключевые слова: ')[1].split('Key words:')[0], \n",
        "                 i.split('Ключевые слова: ')[1].split('Key words:')[1]) for i in papers[:-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBXF0K4W9526"
      },
      "source": [
        "key_words = []\n",
        "cleaned_papers = []\n",
        "for key, paper in key_to_paper:\n",
        "    key = key.strip().strip('.').replace('-\\n', '').replace('\\n', ' ')\n",
        "    key_words.append([k for k in key.split(', ')])\n",
        "    paper = re.sub(r'\\[.+?\\]', '', paper, flags=re.DOTALL)\n",
        "    paper = re.sub(r'\\(.+?\\)', '', paper, flags=re.DOTALL)\n",
        "    paper = re.sub(r'[A-Za-z,;1234567890]', '', paper)\n",
        "    paper = paper.replace('\\n', ' ')\n",
        "    cleaned_papers.append(paper)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esnEZhjg_-Cm"
      },
      "source": [
        "df = pd.DataFrame({'key_words' : [', '.join(key) for key in key_words], \n",
        "                   'papers' : cleaned_papers})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZfF-YXwB5zk"
      },
      "source": [
        "df = df.iloc[16:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTPhm7_7A9ru"
      },
      "source": [
        "df.to_csv('keyword_data.csv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VjXAwBRFkqk"
      },
      "source": [
        "## Keywords Manual Annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlE_Cc8AKNJ1"
      },
      "source": [
        "Я составляла свою разметку на основе ключевых слов, которые были предложены авторами. Из основных систематических изменений были следующие:\n",
        "\n",
        "1. Добавлены аббревиатуры. Часто авторы вводят аббревиатуру в начале статьи и дальше используют только ее, а в ключевых словах указана полная форма (совершенный вид - СВ)\n",
        "\n",
        "2. Все ключевые слова приведены к начальной форме: глаголы –> глагол\n",
        "\n",
        "3. Некоторые ключевые слова объединяли два концепта, я их разделила (сложное и простое предложение –> сложное предложение, простое предложение)\n",
        "\n",
        "Также добавила или убрала несколько, но большую часть слов оставила без изменений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_VrJOcQFsSG"
      },
      "source": [
        "## Automated keyword extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQusQ52DHJYg"
      },
      "source": [
        "df = pd.read_csv('keyword_data.csv', sep='\\t') # В этом файле уже исправленные мною ключевые слова"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPp94RVm-4ji"
      },
      "source": [
        "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D01drNsuIbRt"
      },
      "source": [
        "model = spacy.load('ru_core_news_md')\n",
        "\n",
        "\n",
        "def preprocess(text, model):\n",
        "\n",
        "    docum = model(text)\n",
        "    words = ' '.join([w.lemma_ for w in docum if w.lemma_.isalpha()])\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maZQRzo0KRTC"
      },
      "source": [
        "df['prep_texts'] = df.papers.apply(lambda x: preprocess(x, model))\n",
        "df['prep_kw'] = df.papers.apply(lambda x: preprocess(x, model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QcVg7pOLULf"
      },
      "source": [
        "df['key_count'] = df.key_words.apply(lambda x: len(x.split(', ')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Xy7-X8LmYu"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhXRNYPANu7L"
      },
      "source": [
        "### Rake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o9trTJ1L1XH"
      },
      "source": [
        "stop = stopwords.words('russian') + ['который']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTeVCt11L4aH"
      },
      "source": [
        "rake = RAKE.Rake(stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfId2nzby3Fu"
      },
      "source": [
        "def kw_rake(text, count, rake):\n",
        "    kw = rake.run(text, maxWords=3, minFrequency=3)\n",
        "    kw = [k[0] for k in kw]\n",
        "    return ', '.join(kw[:count])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4M1CSqtL4df"
      },
      "source": [
        "df['rake_raw'] = df.apply(lambda x: kw_rake(x['papers'], x['key_count'], rake), axis=1)\n",
        "df['rake_prep'] = df.apply(lambda x: kw_rake(x['prep_texts'], x['key_count'], rake), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1OeK64NxJU"
      },
      "source": [
        "### TextRank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNEd11PVN3_o"
      },
      "source": [
        "def kw_textrank(text, count):\n",
        "    kw = keywords.keywords(text, language='russian', additional_stopwords=stop, scores=True)[:10]\n",
        "    return ', '.join([k[0] for k in kw])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6YdRcSk0Wg0"
      },
      "source": [
        "df['textrank'] = df.apply(lambda x: kw_textrank(x['prep_texts'], x['key_count']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP7EIn5aREDc"
      },
      "source": [
        "### Embedding-based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3juJdstRHeP"
      },
      "source": [
        "Based on: https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL_E09Zj1dKs"
      },
      "source": [
        "model = SentenceTransformer('sentence-transformers/LaBSE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFPQ5WG5N4J8"
      },
      "source": [
        "def kw_embeddings(text, model, top_n):\n",
        "    n_gram_range = (1, 2)\n",
        "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop).fit([text])\n",
        "    candidates = count.get_feature_names()\n",
        "    doc_embedding = model.encode([text])\n",
        "    candidate_embeddings = model.encode(candidates)\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "    return ', '.join(keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COp7is8oN4WU"
      },
      "source": [
        "df['emb_raw'] = df.apply(lambda x: kw_embeddings(x['prep_texts'], model, x['key_count']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w81wItbN4aJ"
      },
      "source": [
        "df['emb_prep'] = df.apply(lambda x: kw_embeddings(x['papers'], model, x['key_count']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ6GJ_txFxkQ"
      },
      "source": [
        "## Syntactic patterns for keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilMaTWs9F5YZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku7bEvQJF7x1"
      },
      "source": [
        "## Evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzhoC7Q2hgY6"
      },
      "source": [
        "Давайте представим тексты как набор токенов и их сочетаний. Тогда задачу извлечения ключевых слов можно свести к бинарной классификации: каждый элемент набора либо является, либо не является ключевым словом. Исходя из такой трансформации посчитаем метрики"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCrb48plF-zC"
      },
      "source": [
        "df_metrics = pd.DataFrame(columns = ['text_id' , 'extraction_algorithm', 'precision' , 'recall', 'f-score'])\n",
        "\n",
        "for text_id in range(df.shape[0]):\n",
        "    attributes = {column: df.iloc[text_id][column] for column in df if 'raw' not in column}\n",
        "    count = CountVectorizer(ngram_range=(1, 2), stop_words=stop).fit([attributes['papers']])\n",
        "    df_raw_tokens = pd.DataFrame({'tokens' : count.get_feature_names()})\n",
        "    for i in ['prep_kw', 'rake_prep', 'textrank', 'emb_prep']:\n",
        "        df_raw_tokens[i] = df_raw_tokens['tokens'].apply(lambda x: 1 if x in attributes[i] else 0)\n",
        "    for i in ['rake_prep', 'textrank', 'emb_prep']:\n",
        "        precision, recall, fscore, support = precision_recall_fscore_support(df_raw_tokens['prep_kw'], \n",
        "                                                                             df_raw_tokens[i], average='binary')\n",
        "        df_metrics = df_metrics.append(pd.Series((text_id, i, precision, recall, fscore), \n",
        "                                                 index=df_metrics.columns), ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPC-qM1IxawX"
      },
      "source": [
        "cm = sns.light_palette(\"purple\", as_cmap=True)\n",
        "pd.pivot_table(df_metrics, index=['text_id', 'extraction_algorithm']).style.background_gradient(cmap=cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BStLDbZfF_Sa"
      },
      "source": [
        "## Final discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI-RVn3GGCnw"
      },
      "source": [
        "1. ОБЗОР МЕТРИК ПО ТАБЛИЧКЕ\n",
        "\n",
        "2. Судя по всему, существуют более адекватные метрики для извлечения ключевых слов и в датасетах, которые сделаны для этой задачи помимо наличия слова в списке учитывается \"уверенность\" в этом слове, то есть его ранг. Про метрики я читала [тут](https://medium.com/gumgum-tech/exploring-different-keyword-extractors-evaluation-metrics-and-strategies-ef874d336773)\n",
        "\n",
        "3. Надо как-то получше предобработать данные, возможно, попробовать стемминг, потому что в одной из статей писали об обособлении и смысл выражался разными частями речи (обособлять, обособление, обособленный)\n",
        "\n",
        "4. Можно было бы сделать кластеризацию эмбеддингов всех кандидатов на ключевые слова и искать близость не отдельных токенов к тексту, а центроидов или просто средних векторов кластеров. А потом брать слово, которое ближе всего к вектору, репрезентующему кластер\n",
        "\n",
        "5. Вообще эмбеддинги тут хороши, потому что часто авторы статьи выносят в ключевые слова какие-то обобщающие понятия, например, у статьи в журнале может быть тег NLP, но при этом она будет только о NER и сам токен NLP не встретится ни разу. Поэтому, возможно, для метрики качества выделения ключевых слов, возможо, тоже можно пробовать косинусное расстояние, если мы хотим понять, способен ли наш алгоритм выделять ключевые смыслы (которые по сути упрощаются до ключевых слов)"
      ]
    }
  ]
}
